<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Gestura">
  <meta name="keywords" content="Free-Form Gesture, Gesture Understanding, Gesture Intention, Large Vision Language
Model, Multi-Modal">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Gestura: A LVLM-Powered System Bridging Motion and Semantics for Real-Time Free-Form Gesture Understanding</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title"><span style="text-decoration: underline;"> Gestura</span>: A LVLM-Powered System Bridging Motion and Semantics for Real-Time Free-Form Gesture Understanding</h1>
            <div class="is-size-4 publication-authors">
              <span class="author-block">
                Zhuoming Li</a><sup>1*</sup>,</span>
              <span class="author-block">
                Aitong Liu</a><sup>2‚Ä†</sup>,
              </span>
              <span class="author-block">
                Mengxi Jia</a><sup>2</sup>
              </span>
              <span class="author-block">
                <a href="https://github.com/dell-zhang">Dell Zhang</a><sup>2</sup>
              </span>
              <span class="author-block">
                Xuelong Li</a><sup>2</sup>
              </span>
            </div>

            <div class="is-size-4 publication-authors">
              <span class="author-block"><sup>1</sup> Southeast University</span>
              <span class="author-block"><sup>2</sup> Institute of Artificial Intelligence (TeleAI), China
                Telecom</span>
            </div>

            <div class="column has-text-centered">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2504.12704" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/smileformylove/SmartFreeEdit" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/spaces/sunqq/SmartFreeEdit" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-play-circle"></i>
                  </span>
                  <span>Demo</span>
                </a>

              </span>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/spaces/sunqq/SmartFreeEdit" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-play-circle"></i>
                  </span>
                  <span>Model</span>
                </a>

              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video width="100%" controls>
          <source src="./0522.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <h2 class="subtitle has-text-centered">

         contributes a unified framework that leverages structured
hand landmark priors to enhance the semantic reasoning capabilities of LVLM, achieving fine-grained and
interpretable recognition of free-form gestures in open-world scenarios through hierarchical alignment and
progressive training.</h2>
      </div>
    </div>
  </section>

  <section class="section hero is-light" style="margin-top: 40px">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-five-fifths">
          <h2 class="title is-3 has-text-centered"> üé® Results </h2>
          <div class="content has-text-justified">
            <img src="./comparison1.png" width="100%"">
            <p>
            <span style="font-weight: bold;">Comparison between our Gestura and the currently state-of-the-art LVLM models on our Test sets of GestureInt
dataset. Metrics include BLEU-1‚àº4 and ACC (where ACC reports both closed-set / open-set results).
          </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Abstract. -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">üí¨ Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Free-form gesture understanding is highly appealing for human-computer interaction, as it liberates users from the constraints
of predefined gesture categories. However, the sole existing solution‚ÄîGestureGPT‚Äîsuffers from limited recognition accuracy
and slow response times. In this paper, we propose Gestura, an end-to-end system for free-form gesture understanding. Gestura
harnesses a pre-trained Large Vision-Language Model (LVLM) to align the highly dynamic and diverse patterns of free-form
gestures with high-level semantic concepts. To better capture subtle hand movements across different styles, we introduce a
Landmark Processing Module that compensate for LVLMs‚Äô lack of fine-grained domain knowledge by embedding anatomical
hand priors. Further, a Chain-of-Thought (CoT) reasoning strategy enables step-by-step semantic inference, transforming
shallow knowledge into deep semantic understanding and significantly enhancing the model‚Äôs ability to interpret ambiguous
or unconventional gestures. Together, these components allow Gestura to achieve robust and adaptable free-form gesture
comprehension. Additionally, we have developed the first open-source dataset for free-form gesture intention reasoning
and understanding with over 300,000 annotated QA pairs. Experimental results show that Gestura achieves the accuracy of
84.73% (closed-set) / 64.14% (open-set) in the exocentric (third-person) setting and 66.14% (closed-set) / 21.71% (open-set) in the
egocentric (first-person) setting, achieving approximately 20% and 40% higher accuracy on closed-set and open-set tasks,
respectively, compared to GestureGPT. Moreover, Gestura achieves over a 100√ó speedup in response time (1.6 seconds vs.
227 seconds) on an 8B-sized model deployed on a single NVIDIA A100 40GB GPU, and has been validated through real-device
experiments with an edge‚Äìcloud collaborative setup, bringing free-form gesture understanding markedly closer to practical,
real-world deployment. 
            
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!--/ Abstract. -->

  <!-- pipeline -->
  <Section>
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">‚≠êÔ∏è Pipeline</h2>

      <div class="column is-full-width">

        <div class="content has-text-justified">
          <img src="../gesturem.png" width="100%">
          <p>
            Overview of the proposed framework of <span
                style="font-weight: bold;">Gestura</span>. Gestura introduces a hierarchical framework with
two-phase training for free-form gesture understanding. First, the pre-training stage activates the model‚Äôs potential
for free-form generalization by using a multi-view semantic enhancement strategy. In Stage 2, Gestura leverages well-trained
landmark processing module to deliver anatomical and spatial contextual signals to the LVLM backbone, while internalizing
advanced reasoning capabilities through Chain-of-Thought (CoT) tuning. This dual mechanism expands the model‚Äôs reasoning
boundaries, enabling superior generalization in free-form gesture understanding. </p>
        </div>
      </div>
    </div>
  </Section>
  <!-- experiment -->
  <Section>
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">üì¢ Open-world Real-device Experiments</h2>

      <div class="column is-full-width">

        <div class="content has-text-justified">
          <img src="./realenv.png" width="80%">
          <p>
            <span style="font-weight: bold;">Top-1 / Top-3 / Top-5 accuracy per intent category in an open-world experiment.
          </p>
        </div>
      </div>
      <!-- <div class="column is-full-width">

        <div class="content has-text-justified">
          <img src="./brushbench.png">
          <p>
            <span style="font-weight: bold;">Quantitative comparison of SmartFreeEdit with existing methods on BrushBench.
          </p>
        </div>
      </div> -->
    </div>
  </Section>

  

  <!-- BibTeX
  <section class="section" style="background-color: #f1f1f1;" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <p>If you find our work useful, please cite our paper:</p>
      <pre><code>@article{sun2025smartfreeedit,
  title={SmartFreeEdit: Mask-Free Spatial-Aware Image Editing with Complex Instruction Understanding},
  author={Sun, Qianqian and Luo, Jixiang and Zhang, Dell and Li, Xuelong},
  journal={arXiv preprint arXiv:2504.12704},
  year={2025}}
  </code></pre>
    </div>
  </section> -->

  <!-- <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is modified from the <a href="https://nerfies.github.io/">Nerfies</a>, which is licensed
              under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer> -->

</body>

</html>